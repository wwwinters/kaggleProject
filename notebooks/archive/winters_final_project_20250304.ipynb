{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45a5c87-7904-44bc-88ce-5fa377a654b7",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "- Wiley Winters\n",
    "- MSDS 686 Deep Learning\n",
    "- Week 7-8 Kaggle Project&nbsp;&mdash;&nbsp;Brain Tumor Classification\n",
    "- 2025-MAR-09\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05873db-0aad-476d-8b41-bd62b38fbe2c",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5b5d5-8ee4-487d-b812-9e651e8f0411",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "### Required for 80%\n",
    "Complete project on *kaggle.com* using the skills learned in the <u>Deep Learning</u> class.  The following are required:\n",
    "- Show/plot sample images or data with labels\n",
    "- Include at least on of the following\n",
    "  - Convolution\n",
    "  - Max Pooling\n",
    "  - Batch Normalization\n",
    "  - Dropout\n",
    "  - LSTM\n",
    "  - TF-IDf\n",
    "- Use validation data\n",
    "- Evaluate model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333805a-2b73-4173-89af-5e8f15c229dc",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## Additional for another 20%\n",
    "- Use data augmentation\n",
    "- Use at least one of the following:\n",
    "  - Kernels\n",
    "  - Activation functions\n",
    "  - Loss functions\n",
    "  - Libraries\n",
    "  - Methods\n",
    "- Learning rate optimization\n",
    "- Functional API model\n",
    "- Transfer learning with or without trainable parameters\n",
    "- Confusion matrix and / or ROC plots\n",
    "- Plots of accuracy/loss vs epochs\n",
    "- Show/plot sample incorrect prediction with labels and correct label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ff122-b739-4898-b7c2-7da8051f4c91",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------\n",
    "<a name='imports'></a>\n",
    "## 1.0 <span style='color:blue'>|</span> Load Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b571d24-526c-4b90-84b1-d1f484d67a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, logging, random\n",
    "\n",
    "# Data prep and model scoring\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# TensorFlow likes to display a lot of debug information\n",
    "# on my home system\n",
    "# I will squash the messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorFlow').setLevel(logging.FATAL)\n",
    "\n",
    "# tensorflow and keras' API\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Model building\n",
    "from tensorflow.keras import backend, optimizers, regularizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model architecture visualization\n",
    "from visualkeras import layered_view\n",
    "\n",
    "# Model training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "# Make plots have guidelines\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Squash Python warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb16e6f-e27c-4e62-a1e0-e45be18f5204",
   "metadata": {},
   "source": [
    "<a name='random'></a>\n",
    "### 1.1 <span style='color:blue'>|</span> Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbe0b5-e813-4917-aadc-06cad648d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703e0d1-2a4c-40f9-ad69-f8f1a52eaef1",
   "metadata": {},
   "source": [
    "<a name='global'></a>\n",
    "### 1.2 <span style='color:blue'>|</span> Declare Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5498fd7-d5d9-4bce-a59a-8745b6ce98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and testing image directories\n",
    "home_dir = '/home/wiley'\n",
    "trn_dir = home_dir+'/regis/dataScience/kaggleProject/images/data/training'\n",
    "tst_dir = home_dir+'/regis/dataScience/kaggleProject/images/data/testing'\n",
    "\n",
    "# Define classes\n",
    "classes = ['negative', 'positive']\n",
    "\n",
    "# Image size and shape\n",
    "img_size = (224, 224)\n",
    "img_shape = (224, 224, 3)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 2\n",
    "\n",
    "# Declare batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08646792-0d91-4220-8db5-5e36ff566c52",
   "metadata": {},
   "source": [
    "<a name='functions'></a>\n",
    "## 2.0 <span style='color:blue'>|</span> Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd99b9d-667a-4d32-943f-208ca067051a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------\n",
    "<a name='load_df'></a>\n",
    "### 2.1 <span style='color:blue'>|</span> Load DataFrames\n",
    "- Join image filename and path information\n",
    "- Create labels from class directory names\n",
    "- Create dataframe\n",
    "- Randomize dataframe rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215f093-56a5-45bb-acb2-52319ee78c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(path):\n",
    "    # Derive image file paths and labels from directory structure\n",
    "    labels, paths = zip(*[(label, os.path.join(path, label, image))\n",
    "                        for label in os.listdir(path)\n",
    "                        if os.path.isdir(os.path.join(path, label))\n",
    "                        for image in os.listdir(os.path.join(path, label))])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'paths': paths, 'labels': labels})\n",
    "    \n",
    "    # Randomize rows to help eliminate bias\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e05eb6-e0e3-417c-b54f-3addb925c7c0",
   "metadata": {},
   "source": [
    "<a name='metrics'></a>\n",
    "### 2.2 <span style='color:blue'>|</span> Plot Performance Metrics\n",
    "Plot the following:\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Training Accuracy\n",
    "- Validation Accuracy\n",
    "- Training Precision\n",
    "- Validation Precision\n",
    "- Training Recall\n",
    "- Validation Recall\n",
    "- Training AUC\n",
    "- Validation AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72824ec9-23e7-495a-9895-7321b8861f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(epochs, history.history['loss'], 'b', label = 'Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r', label = 'Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'b', label = 'Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'r', label = 'Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.suptitle('Model Loss and Accuracy over Epochs', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation precision\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(epochs, history.history['precision'], 'b', label='Training Precision')\n",
    "    plt.plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')\n",
    "    plt.title('Training and Validation Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation recall\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(epochs, history.history['recall'], 'b', label='Training Recall')\n",
    "    plt.plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')\n",
    "    plt.title('Training and Validation Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle('Model Precision and Recall over Epochs', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation AUC\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(epochs, history.history['auc'], 'b', label='Training AUC')\n",
    "    plt.plot(epochs, history.history['val_auc'], 'r', label='Validation AUC')\n",
    "    plt.title('Training and Validation AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51eb63e-fdba-4eb9-9bf6-7f81d6908e52",
   "metadata": {},
   "source": [
    "<a name='performance'></a>\n",
    "### 2.3 <span style='color:blue'>|</span> Evaluate Model's Performance on Test DataSet\n",
    "- Infer loss, accuracy, precision, recall, and AUC from dataset\n",
    "- Compute F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084c019-8ba9-4c46-a443-b619edb6ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, ds):\n",
    "    # Get metrics from test data\n",
    "    loss, acc, auc, prec, recall = model.evaluate(ds)\n",
    "\n",
    "    # Calculate F1 Score from precision and recall\n",
    "    f1_score = 2 * (prec * recall) / (prec + recall)\n",
    "\n",
    "    # Print results\n",
    "    print('-' * 30)\n",
    "    print(f'Loss:      {loss:.4f}')\n",
    "    print(f'Accuracy:  {acc:.4f}')\n",
    "    print(f'Precision: {prec:.4f}')\n",
    "    print(f'AUC:       {auc:.4f}')\n",
    "    print(f'F1 Score:  {f1_score:.4f}')\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197583d0-9795-48c5-a8f4-47ba3cf21d70",
   "metadata": {},
   "source": [
    "<a name='cm_matrix'></a>\n",
    "### 2.4 <span style='color:blue'>|</span> Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e2ab2-643d-4d87-8ffe-7b3c1f60f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(model, ds):\n",
    "    # Get predictions from dataset\n",
    "    preds = np.argmax(np.round(model.predict(ds)), axis=1)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(ds.classes, preds)\n",
    "\n",
    "    # Visualize confusion matrix\n",
    "    plt.figure(figsize=(5,3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis',\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de302e02-3f44-44d8-a5e9-2c42ca0bf1cc",
   "metadata": {},
   "source": [
    "<a name='tpr'></a>\n",
    "### 2.5 <span style='color:blue'>|</span> Compute TPR and TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09938361-5164-46cf-8dcd-66150a3bf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tpr(model, ds):\n",
    "    # get predictions from dataset\n",
    "    preds = np.argmax(np.round(model.predict(ds)), axis=1)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(ds.classes, preds)\n",
    "\n",
    "    # Extract required values from confusion matrix\n",
    "    (tn, fp, fn, tp) = cm.flatten()\n",
    "\n",
    "    # Calculate TPR\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    # Calculate TNR\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    # Print TPR and TNR\n",
    "    print('-' * 30)\n",
    "    print(f'True Positive Rate (TPR): {tpr:.4f}')\n",
    "    print(f'True Negative Rate (TNR): {tnr:.4f}')\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e51bf-ce68-418a-95d9-e2bd061af0e5",
   "metadata": {},
   "source": [
    "<a name=load_data></a>\n",
    "## 3.0 <span style='color:blue'>|</span> Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e358d5a-b761-42f9-b74d-26e80b587f7d",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "<a name='load_df'></a>\n",
    "### 3.1 <span style='color:blue'>|</span> Create and Load DataFrame for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68e32f-2bd3-45c4-a391-2830c3d44aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "trn_df = load_dataframe(trn_dir)\n",
    "\n",
    "# Load testing data\n",
    "tst_df = load_dataframe(tst_dir)\n",
    "\n",
    "# Take a look at the results\n",
    "print('Training:   \\n', trn_df.head(10).to_markdown())\n",
    "print('Testing:    \\n', tst_df.head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c9db2-c2c8-47bd-b7e8-f8e560893cbb",
   "metadata": {},
   "source": [
    "<a name='eda'></a>\n",
    "## 4.0 <span style='color:blue'>|</span> EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c2877-8ac2-4ec1-9538-058148afa3b4",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "<a name='trn_dist'></a>\n",
    "### 4.1 <span style='color:blue'>|</span> Look at Training Images' Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdfd0c-6a4f-467f-b2db-9660ef784211",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "trn_df['labels'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Image Counts in Training Data')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Image Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83417025-01e6-4455-8da6-4b296105fd5b",
   "metadata": {},
   "source": [
    "Negative images slightly outnumber the positive ones, but are close enough to continue without additional data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb58827-788f-4a7f-b3cc-d3e1c5b1794c",
   "metadata": {},
   "source": [
    "<a name='tst_dist'></a>\n",
    "### 4.2 <span style='color:blue'>|</span> Look at Testing Images' Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f721c78-2fe1-4ead-9893-719aa5358cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "tst_df['labels'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Image Counts in Testing Data')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Image Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45b538-95f6-4941-b788-911279995810",
   "metadata": {},
   "source": [
    "Distribution mirrors what the *training data* shows, but with less frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9268e-735f-4c8f-8eda-b90d7766f17a",
   "metadata": {},
   "source": [
    "<a name='shape'></a>\n",
    "### 4.3 <span style='color:blue'>|</span> Examine Shape of Training and Testing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbc83e-c39f-4fa3-be99-77ce26ea9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Shape: \\n', trn_df.shape)\n",
    "print('Testing Shape:  \\n', tst_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d8aa5-b7de-4a72-9fcc-7621af79763f",
   "metadata": {},
   "source": [
    "**NOTE:**&nbsp;&nbsp;Since the dataframes are built from the contents of the image directories, there should be no missing values or duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e147d77-1ae2-418c-9d9a-943c6529ebc0",
   "metadata": {},
   "source": [
    "<a name='wrangling'></a>\n",
    "## 4.0 <span style='color:blue'>|</span> Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ca6dd-a6b7-4bbb-b493-65a39e414d94",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "<a name='cr_val'></a>\n",
    "### 4.1 <span style='color:blue'>|</span> Create a Validation Subset from Training Data\n",
    "I will use `flow_from_dataframe()` to create datasets for model training; therefore, no reason to create a new directory structure for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f94b2-2e1b-4764-b361-56a1372ae55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df, trn_df = train_test_split(trn_df, train_size=0.2, random_state=42,\n",
    "                                  stratify=trn_df['labels'])\n",
    "print(val_df.sample(10).to_markdown())\n",
    "print(f'Validation Shape: {val_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3952598-74ad-4b34-ac38-6f1738915fff",
   "metadata": {},
   "source": [
    "<a name='proc_imgs'></a>\n",
    "### 4.2 <span style='color:blue'>|</span> Process Images from DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b32df-808b-4c56-a8d9-40b36d7dc911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply image augmentation\n",
    "gen = ImageDataGenerator(rescale=1./255,\n",
    "                         brightness_range=(0.5, 1.5),\n",
    "                         rotation_range=20,\n",
    "                         width_shift_range=0.2,\n",
    "                         height_shift_range=0.2,\n",
    "                         shear_range=0.2,\n",
    "                         zoom_range=0.2)\n",
    "\n",
    "# The test dataset should not be augmented\n",
    "# just rescaled\n",
    "tst_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create training datagen set\n",
    "trn_gen = gen.flow_from_dataframe(trn_df, x_col='paths', y_col='labels',\n",
    "                                  batch_size=batch_size, target_size=img_size,\n",
    "                                  shuffle=True)\n",
    "\n",
    "# Create validation datagen set\n",
    "val_gen = gen.flow_from_dataframe(val_df, x_col='paths', y_col='labels',\n",
    "                                  batch_size=batch_size, target_size=img_size,\n",
    "                                  shuffle=True)\n",
    "\n",
    "# Create test datagen set\n",
    "tst_gen = tst_gen.flow_from_dataframe(tst_df, x_col='paths', y_col='labels',\n",
    "                                      batch_size=16, target_size=img_size,\n",
    "                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8276ee3-e00a-476c-b651-da13679a6b9d",
   "metadata": {},
   "source": [
    "<a name='exam_imgs'></a>\n",
    "### 4.3 <span style='color:blue'>|</span> Examine a few Images and their Labels\n",
    "The images displayed have been augmented in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdbac09-495d-47f5-9ec3-3f4b6cd79282",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = trn_gen.class_indices\n",
    "classes = list(dict.keys())\n",
    "images, labels = next(tst_gen)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i, (image, label) in enumerate(zip(images, labels)):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(image)\n",
    "    class_name = classes[np.argmax(label)]\n",
    "    plt.title(class_name, color='k', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e98e7-2fbf-4d83-8354-0db82190a310",
   "metadata": {},
   "source": [
    "<a name='configure'></a>\n",
    "## 5.0 <span style='color:blue'>|</span> Configure Training Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c43620-17b2-4e09-ae2e-ca3269d6c27c",
   "metadata": {},
   "source": [
    "-----------------------------------------------\n",
    "<a name='basic_values'></a>\n",
    "### 5.1 <span style='color:blue'>|</span> Basic Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8a1dc-5efd-4236-a7a4-3a3fba2af358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 50\n",
    "\n",
    "# Steps per epoch\n",
    "steps_per_ep = trn_gen.samples // batch_size\n",
    "\n",
    "# Validation steps\n",
    "val_steps = tst_gen.samples // batch_size\n",
    "\n",
    "print(f'Image shape:      {img_shape}')\n",
    "print(f'Epochs:           {epochs}')\n",
    "print(f'Batch size:       {batch_size}')\n",
    "print(f'Steps per epoch:  {steps_per_ep}')\n",
    "print(f'Validation steps: {val_steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6217f-fb74-4301-a4d0-f23db810a153",
   "metadata": {},
   "source": [
    "<a name='callbacks'></a>\n",
    "### 5.2 <span style='color:blue'>|</span> Define Callbacks\n",
    "With these *callbacks* the model's training will stop if the loss stops decreasing (`EarlyStopping()`), and the learing rate will be reduced if the validation loss plateaus (`ReduceLROnPlateau()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab248c-be23-4924-aee0-2cd63b9d1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early_stop callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.000000001, patience=4,\n",
    "                           baseline=None, restore_best_weights=True, start_from_epoch=0)\n",
    "\n",
    "# Define reduce LR on Plateau callback\n",
    "reduceLRO = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, mode='auto',\n",
    "                              min_delta=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c5b4f-ca6e-41d6-9923-ea713edb387e",
   "metadata": {},
   "source": [
    "<a name='baseline_model'></a>\n",
    "## 6.0 <span style='color:blue'>|</span> Baseline Model\n",
    "### Define Model's Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3679f1-8937-4c79-975d-74cc2769a616",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "<a name='architecture'></a>\n",
    "### 6.1 <span style='color:blue'>|</span> Define Model's Architecture\n",
    "The functional API method was chosen to define the model's architecture, since it provides a lot of control over the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575ad99-4702-454c-8623-7fea704edcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "\n",
    "inputs  = Input(shape=(img_shape))\n",
    "\n",
    "# Conv Layer 1\n",
    "conv1   = Conv2D(filters=32, kernel_size=4, padding='same',\n",
    "                 activation='relu')(inputs)\n",
    "pool1   = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "drop1   = Dropout(0.1)(pool1)\n",
    "\n",
    "# Conv Layer 2\n",
    "conv2   = Conv2D(filters=64, kernel_size=4, padding='same',\n",
    "                 activation='relu')(drop1)\n",
    "pool2   = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "drop2   = Dropout(0.1)(pool2)\n",
    "\n",
    "# Conv Layer 3\n",
    "conv3   = Conv2D(filters=128, kernel_size=4, padding='same',\n",
    "                 activation='relu')(drop2)\n",
    "pool3   = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "drop3   = Dropout(0.1)(pool3)\n",
    "\n",
    "# Conv Layer 4\n",
    "conv4   = Conv2D(filters=128, kernel_size=4, padding='same',\n",
    "                 activation='relu')(drop3)\n",
    "pool4   = MaxPooling2D(pool_size=(2,2))(conv4)\n",
    "drop4   = Dropout(0.1)(pool4)\n",
    "\n",
    "# Apply Batch Normalization, Flatten, and Dense Layers\n",
    "batch1  = BatchNormalization()(drop4)\n",
    "flatten = Flatten()(batch1)\n",
    "dense1  = Dense(128, activation='relu')(flatten)\n",
    "dropout = Dropout(0.5)(dense1)\n",
    "dense2  = Dense(512, activation='relu')(dropout)\n",
    "\n",
    "# Last Dense layer with softmax activation\n",
    "preds   = Dense(num_classes, activation='softmax')(dense2)\n",
    "\n",
    "# Pulling it all together\n",
    "model_base = Model(inputs, preds)\n",
    "\n",
    "model_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636cf49-9a31-4567-848c-b09fd9158220",
   "metadata": {},
   "source": [
    "<a name='layered_view'></a>\n",
    "### 6.2 <span style='color:blue'>|</span> Visualize Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24608a69-a7c0-49ad-a8f9-262ceff7e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layered_view(model_base, legend=True, max_xy=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c278a-18fd-4b0d-8793-467dbb8d8cf2",
   "metadata": {},
   "source": [
    "<a name='compile'></a>\n",
    "### 6.3 <span style='color:blue'>|</span> Compile and Train Model\n",
    "The `Adam()` optimizer was selected for this model, since it is well suited to classification problems.  The loss function `categorical_crossentropy()` was also selected for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd440b9-60ac-4935-abcf-c0f89d8de63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Adam optimizer\n",
    "opt = optimizers.RMSprop(learning_rate=0.0005)\n",
    "\n",
    "# Compile base model\n",
    "model_base.compile(optimizer=opt, loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy',\n",
    "                            tf.keras.metrics.Precision(name='precision'),\n",
    "                            tf.keras.metrics.Recall(name='recall'),\n",
    "                            tf.keras.metrics.AUC(curve='PR', name='auc')])\n",
    "\n",
    "hist_base = model_base.fit(trn_gen, batch_size=batch_size, steps_per_epoch=steps_per_ep, \n",
    "                           epochs=epochs, validation_data=val_gen,\n",
    "                           validation_steps=val_steps,\n",
    "                           callbacks=[early_stop, reduceLRO])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffd81e-2f3d-4499-9067-ec5e151023e5",
   "metadata": {},
   "source": [
    "<a name='evaluate'></a>\n",
    "## 7.0 <span style='color:blue'>|</span> Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a57c0b-89fb-4591-8f13-8dbfbeebd92f",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "<a name='history'></a>\n",
    "### 7.1 <span style='color:blue'>|</span> Plot Training and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24589b86-f6fd-4b4b-bf69-470cee3e1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a8990-e79e-4a32-a0dc-38338a3fe5a6",
   "metadata": {},
   "source": [
    "<a name='score'></a>\n",
    "### 7.2 <span style='color:blue'>|</span> Score Model\n",
    "To evaluate the model's performance the following matrices will be evaluated against the test dataset:\n",
    "- Model Loss&nbsp;&mdash;&nbsp;gives a nuanced view of model optimization\n",
    "- Model Accuracy&nbsp;&mdash;&nbsp;provides the proportion of all classifications that were correct\n",
    "- Precision&nbsp;&mdash;&nbsp;is the proportion of the model's positive classifications that are actually positive\n",
    "- Recall&nbsp;&mdash;&nbsp;proportion of correct positive classifications\n",
    "- Area Under Curve (AUC)&nbsp;&mdash;&nbsp;represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative\n",
    "- F1 Score&nbsp;&mdash;&nbsp;describes the harmonic mean of the precision and recall of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47a315-a071-4d87-bb2e-7f2f71954199",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model_base, tst_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97560d-e7b9-4803-9f29-05b3e8159315",
   "metadata": {},
   "source": [
    "<a name='plot_cm'></a>\n",
    "### 7.3 <span style='color:blue'>|</span> Plot Confusion Matrix\n",
    "A confusion matrix provides a visual representation of a model's performance when it comes to comparing true positives, false negatives, true negatives, and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c54ea5-9e69-4cce-8c13-f2ba7b4ab992",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(model_base, tst_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f7aaa-6cf3-4a9f-9ef2-33fd8505fbec",
   "metadata": {},
   "source": [
    "<a name=tpr_tnr></a>\n",
    "### 7.4 <span style='color:blue'>|</span> Compute TPR and TNR\n",
    "The True Positive Rate (TPR) and True Negative Rate (TNR) are good indicators of how well the model is predicting positives (1s) and negatives (0s).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1171ac-1f91-4cce-a3bd-aa808e0a5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tpr(model_base, tst_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0b769-f4a8-438a-b2a1-749fcb344c91",
   "metadata": {},
   "source": [
    "<a name='save_weights'></a>\n",
    "## 8.0 <span style='color:blue'>|</span> Save Weights for Future Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902014bb-105c-43ca-9101-fd2502d0d97e",
   "metadata": {},
   "source": [
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298d933-98fe-4fef-86cf-0b26e3d78c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_base.save('kaggleProject.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f70ce0-4829-4892-96f3-a9b53ad76ddc",
   "metadata": {},
   "source": [
    "<a name='dicussion'></a>\n",
    "## 9.0 <span style='color:blue'>|</span> Discussion and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c179e62-515b-4b00-aa76-c9d4387c8006",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "<a name='about_data'></a>\n",
    "### 9.1 <span style='color:blue'>|</span> About the Data\n",
    "A majority of the image data was obtained from Viradiya's Kaggle notebook an it contains two classes of images (2021).  Those with brain tumors and those without.  All of the images are labeled and divide into their respective classes. The *healthy* class was in the minority; therefore additional *healthy* images were copied from a dataset curated by Bhuvaji in 2019. A total of 2,589 healthy and 2,513 tumor images were used for model training.  A traditional 80/20% split was used to separate the images into training and testing datasets.  In addition, another 20% were taken from the training dataset for validation during the training process.</p>\n",
    "Other sources were examined, but discarded due to poor labeling and unknown origin of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17cf99-2c04-4430-a937-26232231740e",
   "metadata": {},
   "source": [
    "<a name='question'></a>\n",
    "### 9.2 <span style='color:blue'>|</span> Research Question\n",
    "A review of literature illustrated that many studies conducted into using machine learning to classify brain tumor MRI images concentrated on classifying images based on the tumor type; however, very few looked at just determining if an image has a tumor or not.</p>\n",
    "The question the author of this study would like it answer is: ***Can a CNN model be developed that can accurately predict if an MRI brain image contains a tumor or not?***  While classifying tumor types is important, many times a tumor is too small to be recognized or a post tumor resection MRI is not clear enough for the radiologist or neurosurgeon to make an accurate diagnoses. This is where using ML/AI to predict if an image is positive for a brain tumor can come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4ca2f-b41c-4101-8a8e-2bc78e09967e",
   "metadata": {},
   "source": [
    "<a name='methods'></a>\n",
    "### 9.3 <span style='color:blue'>|</span> Methods\n",
    "A review of literature was conducted on peer-reviewed articles in search of reliable MRI brain image datasets.  Many papers pointed to the MRI datasets on [kaggle.com](https://www.kaggle.com/search?q=brain+tumor+mri+dataset) as their source.  I found this interesting since a few of the datasets on kaggle.com lack enough documentation to make them useful for research purposes.</p>\n",
    "Once the data was curated, a model architecture had to be decided upon.  Current literature indicated that Convoluted Neural Networks (CNN)s are often used in this type of image classification problem.  Saeedi, et al., suggested an architecture of four convoluted layers each one containing a *MaxPooling2D* and *Dropout* component.  These layers are then flattened and fed into two dense layers for final classification (2023).  This architecture formed the basis for the one used in this study.>/p>\n",
    "### 9.3.1 <span style='color:blue'>|</span> Callbacks\n",
    "To prevent overfitting two callbacks were employed.  The `EarlyStopping()` callback was combined with `ReduceLROnPlateau()` to stop the model's training when validation loss stops decreasing (EarlyStoppping) and the learning rate will be decreased if the validation loss plateaus (ReduceLROnPlateau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da1163-dedf-4b7b-9197-7e311ea35c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
